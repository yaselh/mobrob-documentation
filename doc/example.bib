% Encoding: UTF-8

@inproceedings{He.2016,
 author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
 title = {Deep Residual Learning for Image Recognition},
 pages = {770--778},
 publisher = {IEEE},
 isbn = {978-1-4673-8851-1},
 booktitle = {29th IEEE Conference on Computer Vision and Pattern Recognition},
 year = {2016},
 address = {Piscataway, NJ},
 doi = {10.1109/CVPR.2016.90}
}


@misc{Szegedy.20151211,
 author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
 year = {2015},
 title = {Rethinking the Inception Architecture for Computer Vision},
 url = {http://arxiv.org/pdf/1512.00567}
}

@inproceedings{Dalal.2005,
 author = {Dalal, N. and Triggs, B.},
 title = {Histograms of Oriented Gradients for Human Detection},
 pages = {886--893},
 publisher = {{IEEE Computer Society}},
 isbn = {0-7695-2372-2},
 editor = {Schmid, Cordelia and Tomasi, Carlo and Soatto, Stefano},
 booktitle = {CVPR 2005},
 year = {2005},
 address = {Los Alamitos, Calif},
 doi = {10.1109/CVPR.2005.177}
}


@misc{Dai.20160621,
 author = {Dai, Jifeng and Li, Yi and He, Kaiming and Sun, Jian},
 year = {2016},
 title = {R-FCN: Object Detection via Region-based Fully Convolutional Networks},
 url = {http://arxiv.org/pdf/1605.06409}
}


@article{Ren.2017,
 abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features-using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3] , our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
 author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
 year = {2017},
 title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
 pages = {1137--1149},
 volume = {39},
 number = {6},
 issn = {1939-3539},
 journal = {IEEE transactions on pattern analysis and machine intelligence},
 doi = {10.1109/TPAMI.2016.2577031}
}


% This file was created with Citavi 5.5.0.1

@article{Badrinarayanan.2017,
 abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1] . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet.},
 author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
 year = {2017},
 title = {SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation},
 pages = {2481--2495},
 volume = {39},
 number = {12},
 issn = {1939-3539},
 journal = {IEEE transactions on pattern analysis and machine intelligence},
 doi = {10.1109/TPAMI.2016.2644615}
}


@misc{Chen.20170512,
 author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
 year = {2016},
 title = {DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs},
 url = {http://arxiv.org/pdf/1606.00915}
}


@article{Shelhamer.2017,
 abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build {\dq}fully convolutional{\dq} networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional networks achieve improved segmentation of PASCAL VOC (30{\%} relative improvement to 67.2{\%} mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.},
 author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
 year = {2017},
 title = {Fully Convolutional Networks for Semantic Segmentation},
 pages = {640--651},
 volume = {39},
 number = {4},
 issn = {1939-3539},
 journal = {IEEE transactions on pattern analysis and machine intelligence},
 doi = {10.1109/TPAMI.2016.2572683}
}




@misc{Engelcke.20170305,
 author = {Engelcke, Martin and Rao, Dushyant and Wang, Dominic Zeng and Tong, Chi Hay and Posner, Ingmar},
 year = {2016},
 title = {Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks},
 url = {http://arxiv.org/pdf/1609.06666}
}


@inproceedings{Maturana.2015,
 author = {Maturana, Daniel and Scherer, Sebastian},
 title = {VoxNet: A 3D Convolutional Neural Network for real-time object recognition},
 pages = {922--928},
 publisher = {IEEE},
 isbn = {978-1-4799-9994-1},
 editor = {Burgard, Wolfram},
 booktitle = {2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
 year = {2015},
 address = {Piscataway, NJ},
 doi = {10.1109/IROS.2015.7353481}
}

@InProceedings{quigley2009ros,
  Title                    = {ROS: an open-source Robot Operating System},
  Author                   = {Quigley, Morgan and Conley, Ken and Gerkey, Brian and Faust, Josh and Foote, Tully and Leibs, Jeremy and Wheeler, Rob and Ng, Andrew Y},
  Booktitle                = {ICRA workshop on open source software},
  Year                     = {2009},
  Number                   = {3.2},
  Organization             = {Kobe},
  Pages                    = {5},
  Volume                   = {3}
}

@InProceedings{RRTConnect,
  author    = {J. J. Kuffner and S. M. LaValle},
  title     = {RRT-connect: An efficient approach to single-query path planning},
  booktitle = {Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065)},
  year      = {2000},
  volume    = {2},
  pages     = {995-1001 vol.2},
  doi       = {10.1109/ROBOT.2000.844730},
  issn      = {1050-4729},
  keywords  = {computational geometry;path planning;robots;search problems;3D workspace;6-DOF PUMA arm;7-DOF kinematic chain;RRT-connect;automatic graphic animation;collision-free grasping;collision-free manipulation;collision-free motions;high-dimensional configuration spaces;human arm;randomized algorithm;rapidly-exploring random trees;rigid objects;simple greedy heuristic;single-query path planning;Algorithm design and analysis;Animation;Buildings;Computer science;Humans;Kinematics;Path planning;Robotic assembly;Space exploration;Tree graphs},
}

@TechReport{RRT,
  author = {Steven M. Lavalle},
  title  = {Rapidly-Exploring Random Trees: A New Tool for Path Planning},
  year   = {1998},
}

@Article{PRM,
  author   = {L. E. Kavraki and P. Svestka and J. C. Latombe and M. H. Overmars},
  title    = {Probabilistic roadmaps for path planning in high-dimensional configuration spaces},
  journal  = {IEEE Transactions on Robotics and Automation},
  year     = {1996},
  volume   = {12},
  number   = {4},
  pages    = {566-580},
  month    = {Aug},
  doi      = {10.1109/70.508439},
  issn     = {1042-296X},
  keywords = {graph theory;learning (artificial intelligence);mobile robots;path planning;probability;collision-free configurations;goal configurations;graph;high-dimensional configuration spaces;holonomic robot;learning phase;local planner;motion planning;multi-DOF planar articulated robots;path planning;probabilistic roadmaps;query phase;robots;start configurations;static workspaces;Computer science;Joining processes;Laboratories;Layout;Motion planning;Orbital robotics;Path planning;Robots;Workstations},
}

@Article{SamplingAlgos,
  author        = {Sertac Karaman and Emilio Frazzoli},
  title         = {Sampling-based algorithms for optimal motion planning},
  journal       = {The International Journal of Robotics Research},
  year          = {2011},
  volume        = {30},
  number        = {7},
  pages         = {846-894},
  __markedentry = {[Robin Weitemeyer:6]},
  abstract      = { During the last decade, sampling-based path planning algorithms, such as probabilistic roadmaps (PRM) and rapidly exploring random trees (RRT), have been shown to work well in practice and possess theoretical guarantees such as probabilistic completeness. However, little effort has been devoted to the formal analysis of the quality of the solution returned by such algorithms, e.g. as a function of the number of samples. The purpose of this paper is to fill this gap, by rigorously analyzing the asymptotic behavior of the cost of the solution returned by stochastic sampling-based algorithms as the number of samples increases. A number of negative results are provided, characterizing existing algorithms, e.g. showing that, under mild technical conditions, the cost of the solution returned by broadly used sampling-based algorithms converges almost surely to a non-optimal value. The main contribution of the paper is the introduction of new algorithms, namely, PRM* and RRT*, which are provably asymptotically optimal, i.e. such that the cost of the returned solution converges almost surely to the optimum. Moreover, it is shown that the computational complexity of the new algorithms is within a constant factor of that of their probabilistically complete (but not asymptotically optimal) counterparts. The analysis in this paper hinges on novel connections between stochastic sampling-based path planning algorithms and the theory of random geometric graphs. },
  doi           = {10.1177/0278364911406761},
  eprint        = { https://doi.org/10.1177/0278364911406761 },
  url           = {
        https://doi.org/10.1177/0278364911406761

},
}

@Misc{MoveIt,
  author       = {Ioan A. Sucan and Sachin Chitta},
  title        = {MoveIt! Motion Planning Framework},
  howpublished = {\url{http://moveit.ros.org/}},
  note         = {Accessed: 2018-02-08},
  year={2018},
}

@Misc{MoveItAssistant,
  title        = {MoveIt! Setup Assistant Tutorial},
  howpublished = {\url{http://docs.ros.org/hydro/api/moveit_setup_assistant/html/doc/tutorial.html}},
  note         = {Accessed: 2018-02-08},
    year={2018},
}

@Misc{MoveItCommander,
  author       = {Ioan Sucan},
  title        = {moveit\_commander},
  howpublished = {\url{http://wiki.ros.org/moveit_commander}},
  note         = {Accessed: 2018-02-08},
    year={2018},
}

@Misc{FZIPipeline,
  author       = {Felix Mauch and Pascal Becker},
  title        = {Motion Pipeline},
  howpublished = {\url{https://ids-wiki.fzi.de/index.php/Motion_Pipeline}},
  note         = {Accessed: 2018-02-08},
  year = {2018},
}

@Comment{jabref-meta: databaseType:bibtex;}
